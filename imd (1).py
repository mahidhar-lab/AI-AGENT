# -*- coding: utf-8 -*-
"""imd.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dIj8fWZlADvaQJ2dMfof9shyk6N0cROa
"""

pip install contractions

# Read from file (e.g., dataset_book.txt)
with open('IMDB Dataset.csv', 'r', encoding='utf-8') as file:
    text = file.read()

# Convert all text to lowercase
text_lower = text.lower()

# Print first 500 characters for preview
print(text_lower[:500])

import re

# Read the file content
with open('IMDB Dataset.csv', 'r', encoding='utf-8') as file:
    text = file.read()

# Remove punctuation using regex
text_no_punct = re.sub(r'[^\w\s]', '', text)

# Print the first 500 characters to preview
print(text_no_punct[:50001])

import nltk

nltk.download('stopwords')
from nltk.corpus import stopwords

# Load English stopwords
stop_words = set(stopwords.words('english'))

# Read content from the file
with open('IMDB Dataset.csv', 'r', encoding='utf-8') as file:
    text = file.read()

# Convert to lowercase and split into words
words = text.lower().split()

# Filter out stopwords
filtered = [word for word in words if word not in stop_words]

# Print the first 50 filtered words
print(filtered[:50])

import re

# Read content from the file
with open('IMDB Dataset.csv', 'r', encoding='utf-8') as file:
    text = file.read()

# Remove numbers using regex
text_no_numbers = re.sub(r'\d+', '', text)

# Print the first 500 characters to preview
print(text_no_numbers[:50001])

import re

# Read content from the file
with open('IMDB Dataset.csv', 'r', encoding='utf-8') as file:
    text = file.read()

# Remove URLs using regex
text_no_url = re.sub(r"http\S+|www\S+|https\S+", "", text)

# Print the first 500 characters to preview
print(text_no_url[:50001])





# Read the content from the file
with open('IMDB Dataset.csv', 'r', encoding='utf-8') as file:
    text = file.read()

# Tokenize using split (basic method)
tokens = text.split()

# Print the first 50 tokens
print(tokens[:50001])

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
import re

# Load the IMDB dataset using pandas
df = pd.read_csv("IMDB Dataset.csv")

# Extract review texts and clean HTML tags
docs = [re.sub(r"<.*?>", "", review) for review in df["review"].dropna()]

# Initialize TF-IDF Vectorizer
vectorizer = TfidfVectorizer(stop_words='english', max_features=50001)

# Fit and transform the reviews
X = vectorizer.fit_transform(docs)

# Display vocabulary
print("Vocabulary:")
print(vectorizer.vocabulary_)

# Display TF-IDF matrix shape (avoid printing full matrix for large data)
print("\nTF-IDF Matrix shape:")
print(X.shape)

# Optionally, print TF-IDF vector for first review
print("\nTF-IDF Vector for first review:")
print(X[0])

pip install contractions

import pandas as pd
import contractions

# Load the CSV
df = pd.read_csv("IMDB Dataset.csv")

# Expand contractions in the 'review' column
df["expanded_review"] = df["review"].apply(lambda x: contractions.fix(x) if pd.notnull(x) else "")

# Print a few examples
print(df[["review", "expanded_review"]].head())

import re

with open("IMDB Dataset.csv", "r", encoding="utf-8") as file:
    lines = file.readlines()

# Clean up spaces line by line
cleaned_lines = [re.sub(r'\s+', ' ', line).strip() for line in lines]

# Print cleaned lines
for line in cleaned_lines:
    print(line)